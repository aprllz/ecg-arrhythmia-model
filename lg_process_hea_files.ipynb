{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128979,"status":"ok","timestamp":1719914374127,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"lpahMXFy7rkj","outputId":"e9573654-c472-4004-ab59-b8991549d726"},"outputs":[],"source":["#### FOR ACADEMIC PROJECT WORK\n","#check if lib install or not, if not, install\n","\n","# !pip install wfdb\n","# !pip install lightgbm\n","# !pip install PyWavelets\n","# !pip install biosppy\n","# !pip install torch\n","# !pip install shap"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of files in each folder\n","cpsc_processed: 13757\n","train_dataset: 55649\n","test_dataset: 9248\n"]}],"source":["#unzip the data into the data folder\n","# import zipfile\n","# with zipfile.ZipFile('/workspaces/ecg-arrhythmia-model/data/train_dataset.zip', 'r') as zip_ref:\n","#     zip_ref.extractall('/workspaces/ecg-arrhythmia-model/data/')\n","# import os\n","# #check number of files exist in each data folders\n","# print('Number of files in each folder')\n","# print('cpsc_processed:', len(os.listdir('/workspaces/ecg-arrhythmia-model/data/cpsc_processed')))\n","# print('train_dataset:', len(os.listdir('/workspaces/ecg-arrhythmia-model/data/train_dataset')))\n","# print('test_dataset:', len(os.listdir('/workspaces/ecg-arrhythmia-model/data/test_dataset')))"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6Qq6vcDW7qE2"},"outputs":[],"source":["import re\n","from datetime import datetime\n","import shutil\n","import os\n","from utils import load_dictionary_from_file\n","#load .hea file\n","file_name = 'data\\ptb-xl\\HR00001.hea'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrPV-eoT7qE3"},"outputs":[],"source":["# DEBUG CODE - READ FILES\n","\n","with open(file_name, 'r') as file:\n","    main_content = file.read()\n","# look for contains of the file between tag <code> and </code>\n","# and return the first match\n","# match = re.search(r'<code>(.*?)</code>', html_content, re.DOTALL)\n","if main_content:\n","    print(main_content)\n","else:\n","    print('No match')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpvNDe7Z7qE3"},"outputs":[],"source":["# DEBUG CODE - MODIFY FILES\n","\n","#append date time to first line of match\n","from datetime import datetime\n","now = datetime.now()\n","dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n","# print(dt_string)\n","#split the match into lines\n","lines = main_content.split('\\n')\n","#append date time to first line\n","lines[0] = lines[0] + ' ' + dt_string\n","#in the next 12 lines:\n","\n","for i in range(1, 13):\n","    # lines[i] = lines[i][:12] + lines[i][14:] #remove 'x1' after 12th character\n","    # lines[i] = lines[i][:20] + lines[i][25:] #remove '.0(0)' after 20th character\n","    #get location of '.mat' in the line\n","    mat_loc = lines[i].find('.mat')+4 #location id at beginning of '.mat' -> +4 till the end\n","    # print(mat_loc)\n","    lines[i] = lines[i][:mat_loc+3] + lines[i][mat_loc+5:] #remove 'x1'\n","    lines[i] = lines[i][:mat_loc+11] + lines[i][mat_loc+16:] #remove '.0(0)'\n","\n","#in the remaining lines, replace \"#\" with \"# \"\n","for i in range(13, len(lines)):\n","    lines[i] = lines[i].replace('# ', '#')\n","\n","\n","#join the lines back together\n","new_content = '\\n'.join(lines)\n","print(new_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTAJ8Ujv7qE4"},"outputs":[],"source":["#1. FUNCTION READ, AND MODIFY FILES\n","def process_file(file_name):\n","    with open(file_name, 'r') as file:\n","        main_content = file.read()\n","    # main_content = re.search(r'<code>(.*?)</code>', main_content, re.DOTALL) #no need to get part of the file\n","    file.close()\n","    if main_content:\n","        # lines = main_content.group(1).split('\\n')\n","        lines = main_content.split('\\n')\n","        now = datetime.now()\n","        dt_string = now.strftime(\"%d-%b-%Y %H:%M:%S\")\n","        lines[0] = lines[0] + ' ' + dt_string #append date time to first line\n","        for i in range(1, 13):\n","            #get location of '.mat' in the line\n","            mat_loc = lines[i].find('.mat')+4 #location id at beginning of '.mat' -> +4 till the end\n","            # print(mat_loc)\n","            lines[i] = lines[i][:mat_loc+3] + lines[i][mat_loc+5:] #remove 'x1'\n","            lines[i] = lines[i][:mat_loc+11] + lines[i][mat_loc+16:] #remove '.0(0)'\n","        for i in range(13, len(lines)):\n","            lines[i] = lines[i].replace('# ', '#') #replace \"#\" with \"# \" in the remaining lines\n","        new_content = '\\n'.join(lines)\n","        # new_file_name = file_name.replace('.hea', '_new.hea') #no need to change the file name\n","        #overwrite the original file\n","        with open(file_name, 'w') as file:\n","            file.write(new_content)\n","            file.close()\n","        # print('New file created:', file_name)\n","    else:\n","        print('No match')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbZxi-4_7qE4"},"outputs":[],"source":["# CALL manual (read and modify files)\n","#loop through the files in main folder and process each file\n","files = os.listdir('data\\CPSC_2018')\n","#make full path to each file\n","files = [os.path.join('data\\CPSC_2018', file) for file in files]\n","for file in files:\n","    if file.endswith('.hea'):\n","        process_file(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhYywZz17qE4"},"outputs":[],"source":["#2. FUNCTION MOVE FILES\n","def copy_files(folder, N):\n","    files = os.listdir(folder)\n","    #if N = 0, loop all files\n","    if N == 0:\n","        N = len(files)\n","\n","    for i in range(N):\n","        file_name = os.path.join(folder, files[i])\n","        new_folder = os.path.dirname(folder) #copy to upper folder\n","        # new_folder = os.path.join(os.path.dirname(folder), 'main-data')\n","        shutil.copy(file_name, new_folder)\n","        # print('Copied:', file_name)\n","    print('Done, ', N, 'files copied in', folder)\n","\n","# copy_files('.\\data\\CPSC_2018\\g1', 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mllEww2Y7qE4"},"outputs":[],"source":["#3. FUNCTION MOVE AND PROCESS FILES\n","\n","def main_hea_process(data_folder, N):\n","    #make a list of sub-folders within the main data folder\n","    folders = os.listdir(data_folder)\n","    #loop through the folders and copy N files from each sub-folder\n","    for folder in folders:\n","        folder_name = os.path.join(data_folder, folder)\n","        if os.path.isdir(folder_name): #filter dirs only\n","            copy_files(folder_name, N)\n","    #loop through the files in main folder and process each file\n","    files = os.listdir(data_folder)\n","    #make full path to each file\n","    files = [os.path.join(data_folder, file) for file in files]\n","    for file in files:\n","        if file.endswith('.hea'):\n","            process_file(file)\n","    print('Done processing folders:', data_folder)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-RSU2hU7qE4"},"outputs":[],"source":["#define main data folder string\n","data_folder = 'data\\chapman_shaoxing'\n","# 'data\\ptb-xl'\n","# 'data\\cpsc_2018'\n","# 'data\\cpsc_2018_extra'\n","# 'data\\cpsc_processed'\n","\n","#define samples get from each sub-folder\n","N = 0   #sampling files in each sub-folder.\n","        #Total file = N*number of sub-folder\n","        #N=0, get all files in each sub-folder\n","\n","#process the main data folder\n","main_hea_process(data_folder, N)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YhJ4dQ_7qE4"},"outputs":[],"source":["#Utility function to get unique codes from the files\n","\n","def get_unique_dx_code (data_folder):\n","    files = os.listdir(data_folder)\n","    files = [os.path.join(data_folder, file) for file in files]\n","    list_codes = []\n","    for file in files:\n","        if file.endswith('.hea'):\n","            #read 16th line of the file\n","            with open(file, 'r') as f:\n","                lines = f.readlines()\n","                f.close()\n","            #get the 16th line\n","            line = lines[15]\n","            #get text after '#Dx: '\n","            text = line.split('#Dx: ')[1]\n","            #split the text into list of code, delimited by ','\n","            codes = text.split(',')\n","            #remove leading and trailing white spaces from each code\n","            codes = [code.strip() for code in codes]\n","            #append unique codes to the list\n","            for code in codes:\n","                if code not in list_codes:\n","                    list_codes.append(code)\n","    return list_codes\n","\n","#call the function\n","# data_folder = 'data\\cpsc_2018'\n","list_codes = get_unique_dx_code(data_folder)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9_a_VF_7qE5"},"outputs":[],"source":["import json\n","\n","# Function to load dictionary from file\n","def load_dictionary_from_file(filename):\n","    with open(filename, 'r') as file:\n","        dictionary = json.load(file)\n","    return dictionary\n","\n","# Function to store dictionary to file\n","def store_dictionary_to_file(dictionary, filename):\n","    with open(filename, 'w') as file:\n","        json.dump(dictionary, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45O7Wmhd7qE5"},"outputs":[],"source":["# location of dx_dict file and class_labels file\n","file_dx_name = 'meta_data\\dx_dict.json'  # File name to store the dictionary dx_dict\n","file_class_name = 'meta_data\\class_labels.txt' # File name to store the class labels\n","\n","# if dx_dict can be loaded from file, then load it, else use default dx_dict\n","if os.path.exists(file_dx_name):\n","    dx_dict = load_dictionary_from_file(file_dx_name)\n","else:\n","    #introduce dx_dict to store the codes\n","    dx_dict = {\n","            '426783006': 'SNR', # Normal sinus rhythm\n","            '164889003': 'AF', # Atrial fibrillation\n","            '270492004': 'IAVB', # First-degree atrioventricular block\n","            '164909002': 'LBBB', # Left bundle branch block\n","            '713427006': 'RBBB', # Complete right bundle branch block\n","            '59118001': 'RBBB', # Right bundle branch block\n","            '284470004': 'PAC', # Premature atrial contraction\n","            '63593006': 'PAC', # Supraventricular premature beats\n","            '164884008': 'PVC', # Ventricular ectopics\n","            '429622005': 'STD', # ST-segment depression\n","            '164931005': 'STE', # ST-segment elevation\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9Rb7XLA7qE5"},"outputs":[],"source":["#append the codes to the dictionary\n","for code in list_codes:\n","    if code not in dx_dict:\n","        # print('New code found:', code)\n","        #introduce new id sequence for new codes\n","        seq = 'Other' + str(len(dx_dict)-11+1) #original codes are 11\n","        dx_dict[code] = seq\n","# print(dx_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbJymajf7qE5"},"outputs":[],"source":["#check duplicate codes in dx_dict\n","\n","#make a list of codes\n","codes = list(dx_dict.keys())\n","#make a list of unique codes\n","unique_codes = list(set(codes))\n","#check if the length of the two lists are the same\n","if len(codes) == len(unique_codes):\n","    print('No duplicate codes')\n","else:\n","    print('Duplicate codes found')\n","    #loop through the unique codes\n","    for code in unique_codes:\n","        #count the number of times the code appears in the list of codes\n","        count = codes.count(code)\n","        #if the count is greater than 1, print the code and the count\n","        if count > 1:\n","            print(code, count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zSsuv7K7qE5"},"outputs":[],"source":["# store new dx_dict to file\n","dx_dict = {\n","    '426783006': 'SNR', # Normal sinus rhythm\n","    '164889003': 'AF', # Atrial fibrillation\n","    '270492004': 'IAVB', # First-degree atrioventricular block\n","    '164909002': 'LBBB', # Left bundle branch block\n","    '713427006': 'RBBB', # Complete right bundle branch block\n","    '59118001': 'RBBB', # Right bundle branch block\n","    '284470004': 'PAC', # Premature atrial contraction\n","    '63593006': 'PAC', # Supraventricular premature beats\n","    '164884008': 'PVC', # Ventricular ectopics\n","    '429622005': 'STD', # ST-segment depression\n","    '164931005': 'STE', # ST-segment elevation\n","}\n","# filename = os.path.join(data_folder, filename)\n","store_dictionary_to_file(dx_dict, file_dx_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJFNo_HS7qE5"},"outputs":[],"source":["#define function to make class labels from the dictionary\n","\n","#function to get unique values from the dictionary -> as class labels\n","def get_unique_values(dictionary):\n","    values = list(dictionary.values())\n","    unique_values = list(set(values))\n","    return unique_values\n","\n","#function to store unique values to file\n","def store_class_label_to_file(values, filename):\n","    with open(filename, 'w') as file:\n","        for value in values:\n","            file.write(value + '\\n')\n","        file.close()\n","\n","#function to load unique values from file and store in a list\n","def load_class_label_from_file(filename):\n","    class_label = []\n","    with open(filename, 'r') as file:\n","        for line in file:\n","            class_label.append(line.strip())\n","    return class_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0xy3qTW7qE5"},"outputs":[],"source":["file_class_name = 'meta_data\\class_labels.txt' # File name to store the class labels\n","#get class labels from dictionary\n","\n","class_labels = get_unique_values(dx_dict)\n","#store class labels to file\n","store_class_label_to_file(class_labels, file_class_name)\n","\n","#load dict back to file\n","#dictionary = load_dictionary_from_file(filename)\n","#load class labels back to file\n","#class_labels = load_class_label_from_file(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MLqtpx_7qE7"},"outputs":[],"source":["#load dict back to file\n","dictionary = load_dictionary_from_file(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OY7-gbv17qE7"},"outputs":[],"source":["#Script to execute the above functions\n","\n","# 'data\\ptb-xl'\n","# 'data\\cpsc_2018'\n","# 'data\\cpsc_2018_extra'\n","# 'data\\cpsc_processed'\n","\n","\n","#python preprocess.py --data-dir 'data\\cpsc_2018_extra'\n","#python baselines.py --data-dir 'data\\cpsc_2018_extra'  --classifier 'LR'\n","#python main.py --data-dir 'data\\ptb-xl' --leads 'all' --epochs 2 --use-gpu --batch-size 200\n","#python predict.py --data-dir \"data\\cpsc_processed\" --leads 'all' --use-gpu --batch-size 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w10Gqq7J7qE7"},"outputs":[],"source":["#def function to move files .hea and .mat from one folder to another.\n","#Files name are extracted from 1st column of df\n","def move_files(df, source_folder, dest_folder):\n","    for index, row in df.iterrows():\n","        file_name = row['File']\n","        #append '.hea' and '.mat' to the file name, and move the files, one by one\n","        for ext in ['.hea', '.mat']:\n","            source_file = os.path.join(source_folder, file_name + ext)\n","            dest_file = os.path.join(dest_folder, file_name + ext)\n","            shutil.move(source_file, dest_file)\n","    print('Done moving files')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15477366,"status":"ok","timestamp":1719831334223,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"1ymPHald7qE8","outputId":"91fcae94-e68f-43d4-ba41-73d087dc9c14"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of keep records: 27823 Number of results: 27823 Percentage of keep records: 100.0\n"]}],"source":["# %run \"preprocess.py\" --data-dir \"data/cpsc_processed\" -- done in full\n","# %run \"preprocess.py\" --data-dir \"data/train_dataset\" -- done in full\n","\n","#run function move file and handle .hea files\n","# %run \"preprocess.py\" --data-dir \"data\\chapman_shaoxing\"\n","# %run \"preprocess.py\" --data-dir \"data\\test_dataset\"\n","# %run \"baselines.py\" --data-dir \"data\\train_dataset\" --classifier 'all'\n","# %run \"main.py\" --data-dir \"data\\cpsc_2018_extra\" --leads 'all' --epochs 1 --batch-size 200"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11430211,"status":"ok","timestamp":1719845079614,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"Kj_jtXsmEcXd","outputId":"b6b6ccd3-cd0a-41da-f503-303e6e769fcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["data/cpsc_processed/labels.csv\n","Generating expert features...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6877/6877 [2:58:30<00:00,  1.56s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Start training LR...\n","Finding optimal thresholds on validation dataset...\n","LR F1s: [0.27467811158798283, 0.8108108108108107, 0.8333333333333334, 0.5037037037037038, 0.4673913043478261, 0.6614785992217898, 0.35114503816793896, 0.3, 0.609053497942387]\n","Avg F1: 0.5346215999017525\n","Start training RF...\n","Finding optimal thresholds on validation dataset...\n","RF F1s: [0.3218390804597701, 0.8, 0.9142857142857143, 0.5283018867924528, 0.586046511627907, 0.6733333333333333, 0.3438914027149321, 0.3939393939393939, 0.6]\n","Avg F1: 0.573515258128167\n","Start training LGB...\n","[LightGBM] [Info] Number of positive: 481, number of negative: 5022\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106001 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 160462\n","[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.087407 -> initscore=-2.345716\n","[LightGBM] [Info] Start training from score -2.345716\n","[LightGBM] [Info] Number of positive: 1486, number of negative: 4017\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095689 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 160462\n","[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.270035 -> initscore=-0.994447\n","[LightGBM] [Info] Start training from score -0.994447\n","[LightGBM] [Info] Number of positive: 198, number of negative: 5305\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102300 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 160462\n","[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035980 -> initscore=-3.288138\n","[LightGBM] [Info] Start training from score -3.288138\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Info] Number of positive: 565, number of negative: 4938\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053771 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 160462\n","[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102671 -> initscore=-2.167890\n","[LightGBM] [Info] Start training from score -2.167890\n","[LightGBM] [Info] Number of positive: 694, number of negative: 4809\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.054682 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 160462\n","[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.126113 -> initscore=-1.935772\n","[LightGBM] [Info] Start training from score -1.935772\n","[LightGBM] [Info] Number of positive: 984, number of negative: 4519\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.058147 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 160462\n","[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.178812 -> initscore=-1.524420\n","[LightGBM] [Info] Start training from score -1.524420\n","[LightGBM] [Info] Number of positive: 583, number of negative: 4920\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057667 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 160462\n","[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.105942 -> initscore=-2.132877\n","[LightGBM] [Info] Start training from score -2.132877\n","[LightGBM] [Info] Number of positive: 181, number of negative: 5322\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053984 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 160462\n","[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032891 -> initscore=-3.381107\n","[LightGBM] [Info] Start training from score -3.381107\n","[LightGBM] [Info] Number of positive: 719, number of negative: 4784\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057065 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 160462\n","[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.130656 -> initscore=-1.895171\n","[LightGBM] [Info] Start training from score -1.895171\n","Finding optimal thresholds on validation dataset...\n","LGB F1s: [0.2923076923076923, 0.8225806451612904, 0.9411764705882353, 0.6153846153846153, 0.6794258373205742, 0.7500000000000001, 0.481203007518797, 0.5098039215686274, 0.5833333333333334]\n","Avg F1: 0.6305795025759072\n","Start training MLP...\n","Finding optimal thresholds on validation dataset...\n","MLP F1s: [0.3043478260869565, 0.8254847645429363, 0.787878787878788, 0.5401459854014599, 0.5945945945945946, 0.7510917030567685, 0.4108108108108109, 0.34285714285714286, 0.5873015873015873]\n","Avg F1: 0.571612578059005\n"]}],"source":["%run \"baselines.py\" --data-dir \"data/cpsc_processed\" --classifier 'all'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21186227,"status":"ok","timestamp":1719882929512,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"YBgZml7ZBBWg","outputId":"834cd20e-e1bf-448e-b99c-1285b68f652c"},"outputs":[{"name":"stdout","output_type":"stream","text":["data/train_dataset/labels.csv\n","Generating expert features...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 27823/27823 [5:12:40<00:00,  1.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Start training LR...\n","Finding optimal thresholds on validation dataset...\n","LR F1s: [0.14342629482071714, 0.7941787941787942, 0.8414634146341463, 0.5059101654846335, 0.295, 0.5485519591141398, 0.22141560798548096, 0.29268292682926833, 0.9107049608355091]\n","Avg F1: 0.5059260137647432\n","Start training RF...\n","Finding optimal thresholds on validation dataset...\n","RF F1s: [0.17194570135746606, 0.8353413654618475, 0.8502994011976047, 0.46265060240963857, 0.38674033149171266, 0.6140089418777943, 0.2887139107611548, 0.4736842105263157, 0.9322381930184805]\n","Avg F1: 0.5572914064557793\n","Start training LGB...\n","[LightGBM] [Info] Number of positive: 1053, number of negative: 21204\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234394 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 161751\n","[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047311 -> initscore=-3.002547\n","[LightGBM] [Info] Start training from score -3.002547\n","[LightGBM] [Info] Number of positive: 2018, number of negative: 20239\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245410 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 161751\n","[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.090668 -> initscore=-2.305505\n","[LightGBM] [Info] Start training from score -2.305505\n","[LightGBM] [Info] Number of positive: 648, number of negative: 21609\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.438520 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 161751\n","[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.029114 -> initscore=-3.506974\n","[LightGBM] [Info] Start training from score -3.506974\n","[LightGBM] [Info] Number of positive: 1454, number of negative: 20803\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269688 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 161751\n","[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.065328 -> initscore=-2.660779\n","[LightGBM] [Info] Start training from score -2.660779\n","[LightGBM] [Info] Number of positive: 1555, number of negative: 20702\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258912 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 161751\n","[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069866 -> initscore=-2.588755\n","[LightGBM] [Info] Start training from score -2.588755\n","[LightGBM] [Info] Number of positive: 2286, number of negative: 19971\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259617 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 161751\n","[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.102709 -> initscore=-2.167478\n","[LightGBM] [Info] Start training from score -2.167478\n","[LightGBM] [Info] Number of positive: 1290, number of negative: 20967\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266703 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 161751\n","[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.057959 -> initscore=-2.788308\n","[LightGBM] [Info] Start training from score -2.788308\n","[LightGBM] [Info] Number of positive: 258, number of negative: 21999\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.419860 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 161751\n","[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011592 -> initscore=-4.445793\n","[LightGBM] [Info] Start training from score -4.445793\n","[LightGBM] [Info] Number of positive: 15226, number of negative: 7031\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282315 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 161751\n","[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.684099 -> initscore=0.772676\n","[LightGBM] [Info] Start training from score 0.772676\n","Finding optimal thresholds on validation dataset...\n","LGB F1s: [0.15217391304347827, 0.8798449612403101, 0.8466257668711655, 0.580188679245283, 0.47058823529411764, 0.7033639143730888, 0.2402826855123675, 0.4150943396226415, 0.9435630689206762]\n","Avg F1: 0.5813028404581254\n","Start training MLP...\n","Finding optimal thresholds on validation dataset...\n","MLP F1s: [0.18390804597701152, 0.8435114503816795, 0.7951807228915663, 0.6142857142857143, 0.40109890109890106, 0.6123893805309734, 0.2491803278688524, 0.46875, 0.9230369214768591]\n","Avg F1: 0.5657046071679509\n"]}],"source":["%run \"baselines.py\" --data-dir \"data/train_dataset\" --classifier 'all'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8298817,"status":"ok","timestamp":1719911814058,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"85gpnHUPALkn","outputId":"e5e0a1b9-954e-47b5-e095-53b05d03db7e"},"outputs":[],"source":["#resnet34 \n","%run \"main.py\" --data-dir \"data/cpsc_processed\" --epochs 2 --num-workers 2\n","#--epochs 4 --use-gpu --num-workers 2\n","#--phase 'train' ('test')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"KI4HkmwcBD9o","outputId":"31995817-d956-4ffa-d38e-c41be6c4cb08"},"outputs":[],"source":["%run \"main.py\" --data-dir \"data/train_dataset\" --epochs 2 --num-workers 2 --batch-size 4\n","# --use-gpu"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"not enough values to unpack (expected 3, got 2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m/workspaces/ecg-arrhythmia-model/main.py:113\u001b[0m\n\u001b[1;32m    110\u001b[0m test_data_dir \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/test_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    111\u001b[0m test_label_csv \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_altered.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#check missing class in test set\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m train_folds, val_folds, test_folds \u001b[38;5;241m=\u001b[39m split_data(seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#train_folds, val_folds = split_data(seed=args.seed)\u001b[39;00m\n\u001b[1;32m    116\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ECGDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, data_dir, label_csv, train_folds, leads)\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"]}],"source":["#resnet34 test\n","%run \"main.py\" --data-dir \"data/train_dataset\" --epochs 2 --num-workers 2 --batch-size 8 --phase 'test'\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
