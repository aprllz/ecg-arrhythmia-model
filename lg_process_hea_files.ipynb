{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":128979,"status":"ok","timestamp":1719914374127,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"lpahMXFy7rkj","outputId":"e9573654-c472-4004-ab59-b8991549d726"},"outputs":[],"source":["#### FOR ACADEMIC PROJECT WORK\n","#check if lib install or not, if not, install\n","\n","# !pip install wfdb\n","# !pip install lightgbm\n","# !pip install PyWavelets\n","# !pip install biosppy\n","# !pip install torch\n","# !pip install shap"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#unzip the data into the data folder\n","# import zipfile\n","# with zipfile.ZipFile('/workspaces/ecg-arrhythmia-model/data/train_dataset.zip', 'r') as zip_ref:\n","#     zip_ref.extractall('/workspaces/ecg-arrhythmia-model/data/')\n","# import os\n","# #check number of files exist in each data folders\n","# print('Number of files in each folder')\n","# print('cpsc_processed:', len(os.listdir('/workspaces/ecg-arrhythmia-model/data/cpsc_processed')))\n","# print('train_dataset:', len(os.listdir('/workspaces/ecg-arrhythmia-model/data/train_dataset')))\n","# print('test_dataset:', len(os.listdir('/workspaces/ecg-arrhythmia-model/data/test_dataset')))"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6Qq6vcDW7qE2"},"outputs":[],"source":["import re\n","from datetime import datetime\n","import shutil\n","import os\n","from utils import load_dictionary_from_file\n","#load .hea file\n","file_name = 'data\\ptb-xl\\HR00001.hea'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrPV-eoT7qE3"},"outputs":[],"source":["# DEBUG CODE - READ FILES\n","\n","with open(file_name, 'r') as file:\n","    main_content = file.read()\n","# look for contains of the file between tag <code> and </code>\n","# and return the first match\n","# match = re.search(r'<code>(.*?)</code>', html_content, re.DOTALL)\n","if main_content:\n","    print(main_content)\n","else:\n","    print('No match')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpvNDe7Z7qE3"},"outputs":[],"source":["# DEBUG CODE - MODIFY FILES\n","\n","#append date time to first line of match\n","from datetime import datetime\n","now = datetime.now()\n","dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n","# print(dt_string)\n","#split the match into lines\n","lines = main_content.split('\\n')\n","#append date time to first line\n","lines[0] = lines[0] + ' ' + dt_string\n","#in the next 12 lines:\n","\n","for i in range(1, 13):\n","    # lines[i] = lines[i][:12] + lines[i][14:] #remove 'x1' after 12th character\n","    # lines[i] = lines[i][:20] + lines[i][25:] #remove '.0(0)' after 20th character\n","    #get location of '.mat' in the line\n","    mat_loc = lines[i].find('.mat')+4 #location id at beginning of '.mat' -> +4 till the end\n","    # print(mat_loc)\n","    lines[i] = lines[i][:mat_loc+3] + lines[i][mat_loc+5:] #remove 'x1'\n","    lines[i] = lines[i][:mat_loc+11] + lines[i][mat_loc+16:] #remove '.0(0)'\n","\n","#in the remaining lines, replace \"#\" with \"# \"\n","for i in range(13, len(lines)):\n","    lines[i] = lines[i].replace('# ', '#')\n","\n","\n","#join the lines back together\n","new_content = '\\n'.join(lines)\n","print(new_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTAJ8Ujv7qE4"},"outputs":[],"source":["#1. FUNCTION READ, AND MODIFY FILES\n","def process_file(file_name):\n","    with open(file_name, 'r') as file:\n","        main_content = file.read()\n","    # main_content = re.search(r'<code>(.*?)</code>', main_content, re.DOTALL) #no need to get part of the file\n","    file.close()\n","    if main_content:\n","        # lines = main_content.group(1).split('\\n')\n","        lines = main_content.split('\\n')\n","        now = datetime.now()\n","        dt_string = now.strftime(\"%d-%b-%Y %H:%M:%S\")\n","        lines[0] = lines[0] + ' ' + dt_string #append date time to first line\n","        for i in range(1, 13):\n","            #get location of '.mat' in the line\n","            mat_loc = lines[i].find('.mat')+4 #location id at beginning of '.mat' -> +4 till the end\n","            # print(mat_loc)\n","            lines[i] = lines[i][:mat_loc+3] + lines[i][mat_loc+5:] #remove 'x1'\n","            lines[i] = lines[i][:mat_loc+11] + lines[i][mat_loc+16:] #remove '.0(0)'\n","        for i in range(13, len(lines)):\n","            lines[i] = lines[i].replace('# ', '#') #replace \"#\" with \"# \" in the remaining lines\n","        new_content = '\\n'.join(lines)\n","        # new_file_name = file_name.replace('.hea', '_new.hea') #no need to change the file name\n","        #overwrite the original file\n","        with open(file_name, 'w') as file:\n","            file.write(new_content)\n","            file.close()\n","        # print('New file created:', file_name)\n","    else:\n","        print('No match')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbZxi-4_7qE4"},"outputs":[],"source":["# CALL manual (read and modify files)\n","#loop through the files in main folder and process each file\n","files = os.listdir('data\\CPSC_2018')\n","#make full path to each file\n","files = [os.path.join('data\\CPSC_2018', file) for file in files]\n","for file in files:\n","    if file.endswith('.hea'):\n","        process_file(file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhYywZz17qE4"},"outputs":[],"source":["#2. FUNCTION MOVE FILES\n","def copy_files(folder, N):\n","    files = os.listdir(folder)\n","    #if N = 0, loop all files\n","    if N == 0:\n","        N = len(files)\n","\n","    for i in range(N):\n","        file_name = os.path.join(folder, files[i])\n","        new_folder = os.path.dirname(folder) #copy to upper folder\n","        # new_folder = os.path.join(os.path.dirname(folder), 'main-data')\n","        shutil.copy(file_name, new_folder)\n","        # print('Copied:', file_name)\n","    print('Done, ', N, 'files copied in', folder)\n","\n","# copy_files('.\\data\\CPSC_2018\\g1', 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mllEww2Y7qE4"},"outputs":[],"source":["#3. FUNCTION MOVE AND PROCESS FILES\n","\n","def main_hea_process(data_folder, N):\n","    #make a list of sub-folders within the main data folder\n","    folders = os.listdir(data_folder)\n","    #loop through the folders and copy N files from each sub-folder\n","    for folder in folders:\n","        folder_name = os.path.join(data_folder, folder)\n","        if os.path.isdir(folder_name): #filter dirs only\n","            copy_files(folder_name, N)\n","    #loop through the files in main folder and process each file\n","    files = os.listdir(data_folder)\n","    #make full path to each file\n","    files = [os.path.join(data_folder, file) for file in files]\n","    for file in files:\n","        if file.endswith('.hea'):\n","            process_file(file)\n","    print('Done processing folders:', data_folder)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-RSU2hU7qE4"},"outputs":[],"source":["#define main data folder string\n","data_folder = 'data\\chapman_shaoxing'\n","# 'data\\ptb-xl'\n","# 'data\\cpsc_2018'\n","# 'data\\cpsc_2018_extra'\n","# 'data\\cpsc_processed'\n","\n","#define samples get from each sub-folder\n","N = 0   #sampling files in each sub-folder.\n","        #Total file = N*number of sub-folder\n","        #N=0, get all files in each sub-folder\n","\n","#process the main data folder\n","main_hea_process(data_folder, N)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YhJ4dQ_7qE4"},"outputs":[],"source":["#Utility function to get unique codes from the files\n","\n","def get_unique_dx_code (data_folder):\n","    files = os.listdir(data_folder)\n","    files = [os.path.join(data_folder, file) for file in files]\n","    list_codes = []\n","    for file in files:\n","        if file.endswith('.hea'):\n","            #read 16th line of the file\n","            with open(file, 'r') as f:\n","                lines = f.readlines()\n","                f.close()\n","            #get the 16th line\n","            line = lines[15]\n","            #get text after '#Dx: '\n","            text = line.split('#Dx: ')[1]\n","            #split the text into list of code, delimited by ','\n","            codes = text.split(',')\n","            #remove leading and trailing white spaces from each code\n","            codes = [code.strip() for code in codes]\n","            #append unique codes to the list\n","            for code in codes:\n","                if code not in list_codes:\n","                    list_codes.append(code)\n","    return list_codes\n","\n","#call the function\n","# data_folder = 'data\\cpsc_2018'\n","list_codes = get_unique_dx_code(data_folder)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9_a_VF_7qE5"},"outputs":[],"source":["import json\n","\n","# Function to load dictionary from file\n","def load_dictionary_from_file(filename):\n","    with open(filename, 'r') as file:\n","        dictionary = json.load(file)\n","    return dictionary\n","\n","# Function to store dictionary to file\n","def store_dictionary_to_file(dictionary, filename):\n","    with open(filename, 'w') as file:\n","        json.dump(dictionary, file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45O7Wmhd7qE5"},"outputs":[],"source":["# location of dx_dict file and class_labels file\n","file_dx_name = 'meta_data\\dx_dict.json'  # File name to store the dictionary dx_dict\n","file_class_name = 'meta_data\\class_labels.txt' # File name to store the class labels\n","\n","# if dx_dict can be loaded from file, then load it, else use default dx_dict\n","if os.path.exists(file_dx_name):\n","    dx_dict = load_dictionary_from_file(file_dx_name)\n","else:\n","    #introduce dx_dict to store the codes\n","    dx_dict = {\n","            '426783006': 'SNR', # Normal sinus rhythm\n","            '164889003': 'AF', # Atrial fibrillation\n","            '270492004': 'IAVB', # First-degree atrioventricular block\n","            '164909002': 'LBBB', # Left bundle branch block\n","            '713427006': 'RBBB', # Complete right bundle branch block\n","            '59118001': 'RBBB', # Right bundle branch block\n","            '284470004': 'PAC', # Premature atrial contraction\n","            '63593006': 'PAC', # Supraventricular premature beats\n","            '164884008': 'PVC', # Ventricular ectopics\n","            '429622005': 'STD', # ST-segment depression\n","            '164931005': 'STE', # ST-segment elevation\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9Rb7XLA7qE5"},"outputs":[],"source":["#append the codes to the dictionary\n","for code in list_codes:\n","    if code not in dx_dict:\n","        # print('New code found:', code)\n","        #introduce new id sequence for new codes\n","        seq = 'Other' + str(len(dx_dict)-11+1) #original codes are 11\n","        dx_dict[code] = seq\n","# print(dx_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbJymajf7qE5"},"outputs":[],"source":["#check duplicate codes in dx_dict\n","\n","#make a list of codes\n","codes = list(dx_dict.keys())\n","#make a list of unique codes\n","unique_codes = list(set(codes))\n","#check if the length of the two lists are the same\n","if len(codes) == len(unique_codes):\n","    print('No duplicate codes')\n","else:\n","    print('Duplicate codes found')\n","    #loop through the unique codes\n","    for code in unique_codes:\n","        #count the number of times the code appears in the list of codes\n","        count = codes.count(code)\n","        #if the count is greater than 1, print the code and the count\n","        if count > 1:\n","            print(code, count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zSsuv7K7qE5"},"outputs":[],"source":["# store new dx_dict to file\n","dx_dict = {\n","    '426783006': 'SNR', # Normal sinus rhythm\n","    '164889003': 'AF', # Atrial fibrillation\n","    '270492004': 'IAVB', # First-degree atrioventricular block\n","    '164909002': 'LBBB', # Left bundle branch block\n","    '713427006': 'RBBB', # Complete right bundle branch block\n","    '59118001': 'RBBB', # Right bundle branch block\n","    '284470004': 'PAC', # Premature atrial contraction\n","    '63593006': 'PAC', # Supraventricular premature beats\n","    '164884008': 'PVC', # Ventricular ectopics\n","    '429622005': 'STD', # ST-segment depression\n","    '164931005': 'STE', # ST-segment elevation\n","}\n","# filename = os.path.join(data_folder, filename)\n","store_dictionary_to_file(dx_dict, file_dx_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJFNo_HS7qE5"},"outputs":[],"source":["#define function to make class labels from the dictionary\n","\n","#function to get unique values from the dictionary -> as class labels\n","def get_unique_values(dictionary):\n","    values = list(dictionary.values())\n","    unique_values = list(set(values))\n","    return unique_values\n","\n","#function to store unique values to file\n","def store_class_label_to_file(values, filename):\n","    with open(filename, 'w') as file:\n","        for value in values:\n","            file.write(value + '\\n')\n","        file.close()\n","\n","#function to load unique values from file and store in a list\n","def load_class_label_from_file(filename):\n","    class_label = []\n","    with open(filename, 'r') as file:\n","        for line in file:\n","            class_label.append(line.strip())\n","    return class_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0xy3qTW7qE5"},"outputs":[],"source":["file_class_name = 'meta_data\\class_labels.txt' # File name to store the class labels\n","#get class labels from dictionary\n","\n","class_labels = get_unique_values(dx_dict)\n","#store class labels to file\n","store_class_label_to_file(class_labels, file_class_name)\n","\n","#load dict back to file\n","#dictionary = load_dictionary_from_file(filename)\n","#load class labels back to file\n","#class_labels = load_class_label_from_file(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MLqtpx_7qE7"},"outputs":[],"source":["#load dict back to file\n","dictionary = load_dictionary_from_file(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OY7-gbv17qE7"},"outputs":[],"source":["#Script to execute the above functions\n","\n","# 'data\\ptb-xl'\n","# 'data\\cpsc_2018'\n","# 'data\\cpsc_2018_extra'\n","# 'data\\cpsc_processed'\n","\n","\n","#python preprocess.py --data-dir 'data\\cpsc_2018_extra'\n","#python baselines.py --data-dir 'data\\cpsc_2018_extra'  --classifier 'LR'\n","#python main.py --data-dir 'data\\ptb-xl' --leads 'all' --epochs 2 --use-gpu --batch-size 200\n","#python predict.py --data-dir \"data\\cpsc_processed\" --leads 'all' --use-gpu --batch-size 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w10Gqq7J7qE7"},"outputs":[],"source":["#def function to move files .hea and .mat from one folder to another.\n","#Files name are extracted from 1st column of df\n","def move_files(df, source_folder, dest_folder):\n","    for index, row in df.iterrows():\n","        file_name = row['File']\n","        #append '.hea' and '.mat' to the file name, and move the files, one by one\n","        for ext in ['.hea', '.mat']:\n","            source_file = os.path.join(source_folder, file_name + ext)\n","            dest_file = os.path.join(dest_folder, file_name + ext)\n","            shutil.move(source_file, dest_file)\n","    print('Done moving files')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15477366,"status":"ok","timestamp":1719831334223,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"1ymPHald7qE8","outputId":"91fcae94-e68f-43d4-ba41-73d087dc9c14"},"outputs":[],"source":["# %run \"preprocess.py\" --data-dir \"data/cpsc_processed\" -- done in full\n","# %run \"preprocess.py\" --data-dir \"data/train_dataset\" -- done in full\n","\n","#run function move file and handle .hea files\n","# %run \"preprocess.py\" --data-dir \"data\\chapman_shaoxing\"\n","# %run \"preprocess.py\" --data-dir \"data\\test_dataset\"\n","# %run \"baselines.py\" --data-dir \"data\\train_dataset\" --classifier 'all'\n","# %run \"main.py\" --data-dir \"data\\cpsc_2018_extra\" --leads 'all' --epochs 1 --batch-size 200"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11430211,"status":"ok","timestamp":1719845079614,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"Kj_jtXsmEcXd","outputId":"b6b6ccd3-cd0a-41da-f503-303e6e769fcf"},"outputs":[],"source":["%run \"baselines.py\" --data-dir \"data/cpsc_processed\" --classifier 'all'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21186227,"status":"ok","timestamp":1719882929512,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"YBgZml7ZBBWg","outputId":"834cd20e-e1bf-448e-b99c-1285b68f652c"},"outputs":[],"source":["%run \"baselines.py\" --data-dir \"data/train_dataset\" --classifier 'all'"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8298817,"status":"ok","timestamp":1719911814058,"user":{"displayName":"Long Dang","userId":"18213092265440286102"},"user_tz":-180},"id":"85gpnHUPALkn","outputId":"e5e0a1b9-954e-47b5-e095-53b05d03db7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Validating...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 86/86 [01:40<00:00,  1.16s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 10.2333\n","Shape of y_trues: (687, 9)\n","Shape of y_scores: (687, 9)\n","F1s: [0.83478261 0.91964286 0.93333333 0.96969697 0.92520776 0.7804878\n"," 0.82758621 0.83060109 0.66666667]\n","Avg F1: 0.8542\n","AUCs: [0.97800687 0.98790607 0.9947496  0.9991617  0.98735491 0.9456865\n"," 0.97598596 0.96927229 0.89969834]\n","Avg AUC: 0.9709\n"]}],"source":["#resnet34, testset is part of trainset\n","%run \"main.py\" --data-dir \"data/cpsc_processed\" --epochs 2 --num-workers 2 --batch-size 8 --phase 'test'\n","#--epochs 4 --use-gpu --num-workers 2\n","#--phase 'train' ('test')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validating...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 348/348 [06:39<00:00,  1.15s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 33.4373\n","Shape of y_trues: (2783, 9)\n","Shape of y_scores: (2783, 9)\n","F1s: [0.94837873 0.93178037 0.64779874 0.85714286 0.86153846 0.51538462\n"," 0.83589744 0.52117264 0.45454545]\n","Avg F1: 0.7304\n","AUCs: [0.97680563 0.99382199 0.96236108 0.99329558 0.99099187 0.89928892\n"," 0.98389218 0.91224893 0.88609414]\n","Avg AUC: 0.9554\n"]}],"source":["#resnet34 test, testset is part of train_dataset \n","%run \"main.py\" --data-dir \"data/train_dataset\" --epochs 2 --num-workers 2 --batch-size 8 --phase 'test'\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validating...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 348/348 [06:45<00:00,  1.16s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 150.4477\n","Shape of y_trues: (2782, 9)\n","Shape of y_scores: (2782, 9)\n","F1s: [0.81541409 0.93687708 0.63247863 0.76571429 0.70489039 0.67953668\n"," 0.74545455 0.42080378 0.75      ]\n","Avg F1: 0.7168\n","AUCs: [0.86476461 0.99243247 0.93949008 0.97670868 0.97025251 0.85062641\n"," 0.95459687 0.80194205 0.95711364]\n","Avg AUC: 0.9231\n"]}],"source":["#resnet34, testset is new test datase, using labels_altered.csv\n","%run \"main.py\" --data-dir \"data/cpsc_processed\" --epochs 2 --num-workers 2 --batch-size 8 --phase 'test'"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training epoch 0:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2783/2783 [2:54:01<00:00,  3.75s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss: 492.1525\n","Validating...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 696/696 [13:14<00:00,  1.14s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 90.0963\n","Shape of y_trues: (5566, 9)\n","Shape of y_scores: (5566, 9)\n","F1s: [0.92325764 0.8582996  0.5963939  0.84142395 0.8256513  0.41702128\n"," 0.77697842 0.37609329 0.25806452]\n","Avg F1: 0.6526\n","Training epoch 1:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2783/2783 [2:54:19<00:00,  3.76s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss: 333.7152\n","Validating...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 696/696 [13:13<00:00,  1.14s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 75.1440\n","Shape of y_trues: (5566, 9)\n","Shape of y_scores: (5566, 9)\n","F1s: [0.93762158 0.91855584 0.67651403 0.87868852 0.85912698 0.52014652\n"," 0.80046948 0.46054054 0.27826087]\n","Avg F1: 0.7033\n","Training epoch 2:\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2783/2783 [2:54:20<00:00,  3.76s/it]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss: 291.0483\n","Validating...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 696/696 [13:15<00:00,  1.14s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 67.4626\n","Shape of y_trues: (5566, 9)\n","Shape of y_scores: (5566, 9)\n","F1s: [0.94713426 0.9279732  0.65426357 0.84542587 0.86359176 0.51503759\n"," 0.82978723 0.51840491 0.44705882]\n","Avg F1: 0.7276\n"]}],"source":["#resnet34 train, epochs 5, batch size 8, num workers 2\n","%run \"main.py\" --data-dir \"data/train_dataset\" --epochs 3 --num-workers 2 --batch-size 8"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validating...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 578/578 [10:53<00:00,  1.13s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loss: 135.5483\n","Shape of y_trues: (4623, 9)\n","Shape of y_scores: (4623, 9)\n","F1s: [0.90621469 0.94286503 0.73640167 0.57865169 0.76488396 0.56837607\n"," 1.         0.22934888 0.36781609]\n","Avg F1: 0.6772\n"]},{"ename":"ValueError","evalue":"Only one class present in y_true. ROC AUC score is not defined in that case.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m/workspaces/ecg-arrhythmia-model/main.py:157\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     net\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(args\u001b[38;5;241m.\u001b[39mmodel_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspaces/ecg-arrhythmia-model/main.py:94\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataloader, net, args, criterion, device)\u001b[0m\n\u001b[1;32m     92\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(net\u001b[38;5;241m.\u001b[39mstate_dict(), args\u001b[38;5;241m.\u001b[39mmodel_path)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     aucs \u001b[38;5;241m=\u001b[39m \u001b[43mcal_aucs\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_trues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     avg_auc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(aucs)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUCs:\u001b[39m\u001b[38;5;124m'\u001b[39m, aucs)\n","File \u001b[0;32m/workspaces/ecg-arrhythmia-model/utils.py:66\u001b[0m, in \u001b[0;36mcal_aucs\u001b[0;34m(y_trues, y_scores)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcal_aucs\u001b[39m(y_trues, y_scores):\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_trues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:648\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[1;32m    641\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[1;32m    642\u001b[0m         y_true,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_base.py:119\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    117\u001b[0m     y_true_c \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mtake([c], axis\u001b[38;5;241m=\u001b[39mnot_average_axis)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    118\u001b[0m     y_score_c \u001b[38;5;241m=\u001b[39m y_score\u001b[38;5;241m.\u001b[39mtake([c], axis\u001b[38;5;241m=\u001b[39mnot_average_axis)\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m--> 119\u001b[0m     score[c] \u001b[38;5;241m=\u001b[39m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Average the results\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:382\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    387\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."]}],"source":["#resnet34, testset is new test datase, using 'labels.csv'\n","%run \"main.py\" --data-dir \"data/cpsc_processed\" --epochs 2 --num-workers 2 --batch-size 8 --phase 'test'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Validating...\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▍ | 490/578 [09:17<01:40,  1.14s/it]\n"]},{"ename":"RuntimeError","evalue":"Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 173, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/storage.py\", line 913, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/storage.py\", line 269, in _new_shared\n    return cls._new_using_fd_cpu(size)\nRuntimeError: unable to write to file </torch_9890_1790508254_491>: No space left on device (28)\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/workspaces/ecg-arrhythmia-model/main.py:157\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     net\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(args\u001b[38;5;241m.\u001b[39mmodel_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/workspaces/ecg-arrhythmia-model/main.py:56\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataloader, net, args, criterion, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m output_list, labels_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     55\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (data, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader)):\n\u001b[1;32m     57\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# #print round id and label. Modified. Long. 11.Jul.24\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# print('Round:', idx, 'Label:', labels, 'Data:', data)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# idx += 1\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# #end of modification\u001b[39;00m\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 173, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 211, in collate_tensor_fn\n    storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/storage.py\", line 913, in _new_shared\n    untyped_storage = torch.UntypedStorage._new_shared(size * self._element_size(), device=device)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/storage.py\", line 269, in _new_shared\n    return cls._new_using_fd_cpu(size)\nRuntimeError: unable to write to file </torch_9890_1790508254_491>: No space left on device (28)\n"]}],"source":["#resnet34, testset is new test datase, using 'labels.csv'\n","%run \"main.py\" --data-dir \"data/train_dataset\" --epochs 2 --num-workers 2 --batch-size 8 --phase 'test'"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
