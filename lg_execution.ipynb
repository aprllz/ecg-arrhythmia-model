{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for: control data preprocessing and model training, testing, and evaluation\n",
    "# Dataset: data/cpsc_processed: \n",
    "\n",
    "# Dataset: data/op_09_classes/:\n",
    "# - training/evaluation:\n",
    "# - testing:\n",
    "\n",
    "# Dataset: data/op_08_classes (without PVC class in testing data)\n",
    "# - training/evaluation:\n",
    "# - testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for cpsc_processed:\n",
    "\n",
    "#perform preprocessing,baseline on data/cpsc_processed\n",
    "%run \"preprocess.py\" --data-dir \"data/cpsc_processed\" \n",
    "%run \"baselines.py\" --data-dir \"data/cpsc_processed\" --classifier 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for op_09_classes:\n",
    "\n",
    "#perform preprocessing,baseline on the data, op_9_classes\n",
    "%run \"preprocess.py\" --data-dir \"data/op_09_classes/train_dataset\" \n",
    "%run \"preprocess.py\" --data-dir \"data/op_09_classes/test_dataset\"\n",
    "\n",
    "#perform baseline training on the data, op_9_classes\n",
    "%run \"baselines.py\" --data-dir \"data/op_09_classes/train_dataset\" --classifier 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#prepare data for op_08_classes:\n",
    "#reset csv files\n",
    "\n",
    "#path to training data and testing data\n",
    "train_data_dir = \"data/op_08_classes/train_dataset\"\n",
    "test_data_dir = \"data/op_08_classes/test_dataset\"\n",
    "\n",
    "#count existing .csv files in the training data and testing data\n",
    "!ls $train_data_dir/*.csv | wc -l\n",
    "!ls $test_data_dir/*.csv | wc -l\n",
    "\n",
    "#delete all existing .csv files in the training data \n",
    "!rm -rf $train_data_dir/*.csv\n",
    "#delete all existing .csv files in the testing data\n",
    "!rm -rf $test_data_dir/*.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/op_08_classes/train_dataset/labels.csv\n",
      "Generating expert features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27823 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27823/27823 [43:37<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training LR...\n",
      "Finding optimal thresholds on validation dataset...\n",
      "LR F1s: [np.float64(0.9191251271617498), np.float64(0.61), np.float64(0.2242562929061785), np.float64(0.7887323943661971), np.float64(0.8015267175572519), np.float64(0.2), np.float64(0.35452793834296725), np.float64(0.17777777777777778)]\n",
      "Avg F1: 0.5094932810140153\n",
      "Start training RF...\n",
      "Finding optimal thresholds on validation dataset...\n",
      "RF F1s: [np.float64(0.9316493313521546), np.float64(0.6151012891344383), np.float64(0.31868131868131866), np.float64(0.8529411764705882), np.float64(0.8282442748091603), np.float64(0.14393939393939395), np.float64(0.41839080459770117), np.float64(0.46153846153846156)]\n",
      "Avg F1: 0.5713107563154021\n",
      "Start training LGB...\n",
      "[LightGBM] [Info] Number of positive: 15134, number of negative: 7123\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161670\n",
      "[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.679966 -> initscore=0.753615\n",
      "[LightGBM] [Info] Start training from score 0.753615\n",
      "[LightGBM] [Info] Number of positive: 2330, number of negative: 19927\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126776 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161670\n",
      "[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.104686 -> initscore=-2.146207\n",
      "[LightGBM] [Info] Start training from score -2.146207\n",
      "[LightGBM] [Info] Number of positive: 1312, number of negative: 20945\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161670\n",
      "[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.058948 -> initscore=-2.770347\n",
      "[LightGBM] [Info] Start training from score -2.770347\n",
      "[LightGBM] [Info] Number of positive: 668, number of negative: 21589\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161670\n",
      "[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030013 -> initscore=-3.475651\n",
      "[LightGBM] [Info] Start training from score -3.475651\n",
      "[LightGBM] [Info] Number of positive: 1995, number of negative: 20262\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161670\n",
      "[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.089635 -> initscore=-2.318103\n",
      "[LightGBM] [Info] Start training from score -2.318103\n",
      "[LightGBM] [Info] Number of positive: 1037, number of negative: 21220\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.126138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161670\n",
      "[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.046592 -> initscore=-3.018612\n",
      "[LightGBM] [Info] Start training from score -3.018612\n",
      "[LightGBM] [Info] Number of positive: 1587, number of negative: 20670\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161670\n",
      "[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.071303 -> initscore=-2.566838\n",
      "[LightGBM] [Info] Start training from score -2.566838\n",
      "[LightGBM] [Info] Number of positive: 263, number of negative: 21994\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.128701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161670\n",
      "[LightGBM] [Info] Number of data points in the train set: 22257, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011817 -> initscore=-4.426371\n",
      "[LightGBM] [Info] Start training from score -4.426371\n",
      "Finding optimal thresholds on validation dataset...\n",
      "LGB F1s: [np.float64(0.9399699849924963), np.float64(0.6911196911196911), np.float64(0.31529411764705884), np.float64(0.8444444444444444), np.float64(0.8576512455516014), np.float64(0.20161290322580644), np.float64(0.4954545454545455), np.float64(0.4)]\n",
      "Avg F1: 0.5931933665544555\n",
      "Start training MLP...\n",
      "Finding optimal thresholds on validation dataset...\n",
      "MLP F1s: [np.float64(0.928388746803069), np.float64(0.627151051625239), np.float64(0.3254237288135593), np.float64(0.8029197080291971), np.float64(0.8100358422939068), np.float64(0.17100371747211895), np.float64(0.43169398907103823), np.float64(0.22857142857142856)]\n",
      "Avg F1: 0.5406485265849446\n"
     ]
    }
   ],
   "source": [
    "#prepare data for op_08_classes:\n",
    "\n",
    "#perform preprocessing,baseline on the data, op_08_classes\n",
    "%run \"preprocess.py\" --data-dir \"data/op_08_classes/train_dataset\" --num-classes 8\n",
    "%run \"preprocess.py\" --data-dir \"data/op_08_classes/test_dataset\" --num-classes 8\n",
    "\n",
    "#perform baseline training on the data, op_08_classes\n",
    "%run \"baselines.py\" --data-dir \"data/op_08_classes/train_dataset\" --classifier 'all' --num-classes 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cpsc_processed/labels_8_classes.csv\n",
      "Generating expert features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6877/6877 [12:00<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training LR...\n",
      "Finding optimal thresholds on validation dataset...\n",
      "LR F1s: [np.float64(0.5137614678899083), np.float64(0.6058823529411764), np.float64(0.3463203463203463), np.float64(0.6976744186046512), np.float64(0.8663366336633663), np.float64(0.25609756097560976), np.float64(0.5612244897959183), np.float64(0.22641509433962265)]\n",
      "Avg F1: 0.509214045566325\n",
      "Start training RF...\n",
      "Finding optimal thresholds on validation dataset...\n",
      "RF F1s: [np.float64(0.5609756097560976), np.float64(0.6586102719033232), np.float64(0.3541666666666667), np.float64(0.8085106382978723), np.float64(0.8388746803069054), np.float64(0.3026315789473684), np.float64(0.6057142857142858), np.float64(0.47058823529411764)]\n",
      "Avg F1: 0.5750089958608297\n",
      "Start training LGB...\n",
      "[LightGBM] [Info] Number of positive: 730, number of negative: 4773\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 160254\n",
      "[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.132655 -> initscore=-1.877686\n",
      "[LightGBM] [Info] Start training from score -1.877686\n",
      "[LightGBM] [Info] Number of positive: 980, number of negative: 4523\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 160254\n",
      "[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.178085 -> initscore=-1.529378\n",
      "[LightGBM] [Info] Start training from score -1.529378\n",
      "[LightGBM] [Info] Number of positive: 570, number of negative: 4933\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 160254\n",
      "[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103580 -> initscore=-2.158066\n",
      "[LightGBM] [Info] Start training from score -2.158066\n",
      "[LightGBM] [Info] Number of positive: 192, number of negative: 5311\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 160254\n",
      "[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034890 -> initscore=-3.320040\n",
      "[LightGBM] [Info] Start training from score -3.320040\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1468, number of negative: 4035\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 160254\n",
      "[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.266764 -> initscore=-1.011105\n",
      "[LightGBM] [Info] Start training from score -1.011105\n",
      "[LightGBM] [Info] Number of positive: 501, number of negative: 5002\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 160254\n",
      "[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.091041 -> initscore=-2.300987\n",
      "[LightGBM] [Info] Start training from score -2.300987\n",
      "[LightGBM] [Info] Number of positive: 710, number of negative: 4793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 160254\n",
      "[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.129021 -> initscore=-1.909647\n",
      "[LightGBM] [Info] Start training from score -1.909647\n",
      "[LightGBM] [Info] Number of positive: 175, number of negative: 5328\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030622 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 160254\n",
      "[LightGBM] [Info] Number of data points in the train set: 5503, number of used features: 636\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031801 -> initscore=-3.415945\n",
      "[LightGBM] [Info] Start training from score -3.415945\n",
      "Finding optimal thresholds on validation dataset...\n",
      "LGB F1s: [np.float64(0.6162162162162163), np.float64(0.7761194029850746), np.float64(0.36231884057971014), np.float64(0.8163265306122449), np.float64(0.8939393939393939), np.float64(0.3184713375796178), np.float64(0.7058823529411765), np.float64(0.5777777777777777)]\n",
      "Avg F1: 0.6333814815789015\n",
      "Start training MLP...\n",
      "Finding optimal thresholds on validation dataset...\n",
      "MLP F1s: [np.float64(0.5698324022346368), np.float64(0.6785714285714286), np.float64(0.41284403669724773), np.float64(0.8163265306122449), np.float64(0.8777777777777778), np.float64(0.3076923076923077), np.float64(0.593939393939394), np.float64(0.36363636363636365)]\n",
      "Avg F1: 0.5775775301451751\n"
     ]
    }
   ],
   "source": [
    "#prepare data for cpsc_processed FOR (8 classes):\n",
    "\n",
    "#perform preprocessing,baseline on the data, cpsc_processed\n",
    "%run \"preprocess.py\" --data-dir \"data/cpsc_processed\" --num-classes 8\n",
    "\n",
    "\n",
    "#perform baseline training on the data, cpsc_processed\n",
    "%run \"baselines.py\" --data-dir \"data/cpsc_processed\" --classifier 'all' --num-classes 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model, using train data: data/cpsc_processed, with full 09 classes:\n",
    "%run \"main.py\" --data-dir \"data/cpsc_processed\" --epochs 5 --num-workers 2 --batch-size 8 --num-classes 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/688 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspaces/ecg-arrhythmia-model/dataset.py\", line 74, in __getitem__\n    labels = row[self.classes].to_numpy(dtype=np.float32)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/series.py\", line 1153, in __getitem__\n    return self._get_with(key)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/series.py\", line 1194, in _get_with\n    return self.loc[key]\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1191, in __getitem__\n    return self._getitem_axis(maybe_callable, axis=axis)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1420, in _getitem_axis\n    return self._getitem_iterable(key, axis=axis)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1360, in _getitem_iterable\n    keyarr, indexer = self._get_listlike_indexer(key, axis)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1558, in _get_listlike_indexer\n    keyarr, indexer = ax._get_indexer_strict(key, axis_name)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 6200, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 6252, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['PVC'] not in index\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/ecg-arrhythmia-model/main.py:167\u001b[0m\n\u001b[1;32m    165\u001b[0m         net\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(args\u001b[38;5;241m.\u001b[39mmodel_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 167\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m         evaluate(val_loader, net, args, criterion, device)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/ecg-arrhythmia-model/main.py:37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, net, args, criterion, epoch, scheduler, optimizer, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     36\u001b[0m output_list, labels_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (data, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader)):\n\u001b[1;32m     38\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m     output \u001b[38;5;241m=\u001b[39m net(data)\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/codespace/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/workspaces/ecg-arrhythmia-model/dataset.py\", line 74, in __getitem__\n    labels = row[self.classes].to_numpy(dtype=np.float32)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/series.py\", line 1153, in __getitem__\n    return self._get_with(key)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/series.py\", line 1194, in _get_with\n    return self.loc[key]\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1191, in __getitem__\n    return self._getitem_axis(maybe_callable, axis=axis)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1420, in _getitem_axis\n    return self._getitem_iterable(key, axis=axis)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1360, in _getitem_iterable\n    keyarr, indexer = self._get_listlike_indexer(key, axis)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexing.py\", line 1558, in _get_listlike_indexer\n    keyarr, indexer = ax._get_indexer_strict(key, axis_name)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 6200, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 6252, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['PVC'] not in index\"\n"
     ]
    }
   ],
   "source": [
    "#Train model, using train data: data/cpsc_processed, with only 08 classes:\n",
    "%run \"main.py\" --data-dir \"data/cpsc_processed\" --epochs 5 --num-workers 2 --batch-size 8 --num-classes 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
